{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face에서 발급받은 토큰 입력\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "# Hugging Face API 토큰 설정\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인 및 설정\n",
    "# GPU가 사용 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU를 사용\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# 로컬 경로 설정\n",
    "MODEL_PATH = \"./model\"  # 모델이 다운로드된 경로\n",
    "DATASET_PATH = \"./dataset\"  # 데이터셋이 다운로드된 경로\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device) # 모델을 GPU/CPU로 로드\n",
    "\n",
    "# 데이터셋 로드\n",
    "# dataset = load_dataset(DATASET_PATH)\n",
    "dataset = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 크기 감소 함수 정의\n",
    "# 전체 데이터셋 크기를 줄여 학습 시간을 단축 (기본: 10%)\n",
    "def sample_dataset(dataset, fraction=0.1):\n",
    "    return dataset.shuffle(seed=42).select(range(int(len(dataset) * fraction)))\n",
    "\n",
    "# 학습(train) 및 검증(validation) 데이터셋 샘플링\n",
    "train_dataset = sample_dataset(dataset['train'], fraction=0.05)    # 학습 데이터의 5%만 사용\n",
    "val_dataset = sample_dataset(dataset['validation'], fraction=0.05) # 검증 데이터의 5%만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 정의\n",
    "# 모델 입력과 타겟(summary)을 토크나이저로 처리 (길이를 맞추기 위해 패딩과 트렁케이션 적용)\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples['document'] # 본문\n",
    "    targets = examples['summary'] # 요약문\n",
    "    \n",
    "    # 입력 텍스트를 모델에 맞게 토큰화, 최대 길이로 패딩 추가\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 타겟 텍스트도 동일하게 토큰화 및 패딩\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=128, truncation=True, padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    # 입력 텍스트와 타겟 텍스트를 매핑\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터 전처리 함수 적용 (batch 단위로 처리)\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 컬럼(document와 summary) 제거\n",
    "train_dataset = train_dataset.remove_columns(['document', 'summary'])\n",
    "val_dataset = val_dataset.remove_columns(['document', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정 정의\n",
    "# 모델 학습에 필요한 다양한 설정을 포함\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",        # 학습 결과를 저장할 디렉토리\n",
    "    evaluation_strategy=\"epoch\",   # 매 에포크마다 검증 실행\n",
    "    learning_rate=5e-5,            # 학습률 설정\n",
    "    per_device_train_batch_size=8, # GPU 한 대당 학습 배치 크기\n",
    "    per_device_eval_batch_size=8,  # GPU 한 대당 검증 배치 크기\n",
    "    num_train_epochs=3,            # 학습 에포크 수\n",
    "    save_strategy=\"epoch\",         # 매 에포크마다 모델 저장\n",
    "    logging_dir=\"./logs\",          # 로그를 저장할 디렉토리\n",
    "    logging_strategy=\"steps\",      # 몇 스텝마다 로그 저장\n",
    "    logging_steps=100,             # 100 스텝마다 로그 기록\n",
    "    save_total_limit=2,            # 저장할 체크포인트 파일 수 제한\n",
    "    predict_with_generate=True,    # 검증 시 요약 생성 활성화\n",
    "    fp16=True,                     # Mixed Precision 학습 활성화 (FP16)\n",
    "    push_to_hub=False              # Hugging Face Hub로 푸시 비활성화\n",
    ")\n",
    "\n",
    "# Trainer 초기화\n",
    "# 모델, 데이터셋, 설정을 Trainer 객체로 전달\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                   # 학습시킬 모델\n",
    "    args=training_args,            # 학습 설정\n",
    "    train_dataset=train_dataset,   # 학습 데이터셋\n",
    "    eval_dataset=val_dataset,      # 검증 데이터셋\n",
    "    tokenizer=tokenizer            # 토크나이저\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "# Trainer 객체의 train() 메소드를 호출하여 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 완료 후 모델 저장\n",
    "trainer.save_model(\"./fine_tuned_model\")        # Fine-tuned 모델 저장\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\") # 토크나이저 저장\n",
    "\n",
    "print(\"Fine-tuned model saved to './fine_tuned_model'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
