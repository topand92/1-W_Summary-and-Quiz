{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 KoT5_news_summarization 모델을 사용하여 요약합니다.\n",
    "\n",
    "    매개변수:\n",
    "        input_text (str): 요약할 원본 텍스트.\n",
    "\n",
    "    반환값:\n",
    "        str: 요약된 텍스트.\n",
    "    \"\"\"\n",
    "    # 로컬에 저장된 모델 경로 설정 (필요 시 경로 변경)\n",
    "    model_path = \"./model\"  \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)  # 토크나이저 로드\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)  # 모델 로드\n",
    "\n",
    "    # 입력 텍스트를 토큰화 (최대 길이 512로 제한)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # 요약 수행 (생성된 텍스트의 최소 및 최대 길이 조정 가능)\n",
    "    summary_ids = model.generate(\n",
    "        inputs, \n",
    "        max_length=150,  # 요약의 최대 길이\n",
    "        min_length=40,   # 요약의 최소 길이\n",
    "        length_penalty=2.0,  # 길이 패널티 설정 (높을수록 짧은 요약)\n",
    "        num_beams=4,  # 빔 서치 알고리즘 사용 (값이 클수록 성능 향상 가능)\n",
    "        early_stopping=True  # 적절한 지점에서 요약 중단\n",
    "    )\n",
    "\n",
    "    # 요약된 결과를 디코딩하여 반환\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 결과: 젠슨 황 엔비디아 최고경영자(CEO)는 지난 7일 미국 라스베이거스 ‘시이에스 2025’ 현장에서 진행된 블룸버그와의 인터뷰에서 “로봇은 다양한 분야에 적용될 수 있다. 제품 생산 분야에 가장 먼저 적용될 것”이라며 로봇은 다양한 분야에 적용될 수 있다. 제품 생산 분야에 가장 먼저 적용될 것”이라며 로봇은 다양한 분야에 적용될 수 있다. 제품 생산\n"
     ]
    }
   ],
   "source": [
    "input_text = \"로보틱스 시장 규모는 50조달러(약 7경2800조원)로 평가됩니다. 앞으로 더 커질 거대한 시장입니다.” 젠슨 황 엔비디아 최고경영자(CEO)는 지난 7일(현지시각) 미국 라스베이거스 ‘시이에스(CES) 2025’ 현장에서 진행된 블룸버그와의 인터뷰에서 “로봇은 다양한 분야에 적용될 수 있다. 제품 생산 분야에 가장 먼저 적용될 것”이라며 이렇게 말했다. 전날 기조연설에서 발표한 ‘피지컬 에이아이(AI·인공지능)’의 중요성을 강조하며 시장 기회가 열리고 있음을 알린 것이다. ‘피지컬 에이아이’는 젠슨 황이 제시한 인공지능 발전 ​단계 중 네번째 단계에 해당하는 개념이다. 텍스트, 이미지 같은 데이터를 이해하고 처리하는 ‘인식 에이아이’, 데이터에서 패턴을 파악해 콘텐츠를 생성할 수 있는 ‘생성형 에이아이’, 사람의 개입 없이 특정 작업을 자율 수행하는 ‘에이전트 에이아이’를 거쳐, 이젠 물리 법칙까지 이해하는 ‘피지컬 에이아이’ 시대가 시작됐다는 것이다. 피지컬 에이아이는 인터넷, 피시(PC), 스마트폰을 벗어난 인공지능이란 점에서 기술·산업적으로 큰 의미를 지닌다. 물리 법칙을 이해한다는 건 단순히 에이아이와 대화를 나누는 수준을 넘어 현실 세계에서 에이아이에게 작업을 시킬 수 있다는 뜻이 되기 때문이다. 자율주행차, 공장 생산 라인에 배치할 수 있는 휴머노이드 로봇 등이 대표적인 예다. 다양한 물리 기기에 탑재된 피지컬 에이아이가 제조, 물류, 건설, 에너지, 농업, 식품, 의료, 서비스 산업에 이르기까지 사실상 모든 산업에 활용될 것이란 관측이 나온다. 테슬라는 생산 라인에 배치하기 위해 휴머노이드 로봇 ‘옵티머스’를 개발 중이며 베엠베(BMW) 역시 미국 스타트업 피겨와 손잡고 휴머노이드 로봇 ‘피겨 02’를 생산 공정에 투입해 테스트를 진행하고 있다. 문제는 방대한 양의 실제 데이터와 함께 테스트가 필요하다 보니 피지컬 에이아이를 구축하는 비용이 만만치 않다는 점이다. 엔비디아는 이 문제를 해결하겠다며 ‘코스모스’라는 새로운 피지컬 에이아이 개발 플랫폼을 발표했다. 엔비디아에 따르면 ‘코스모스 월드 파운데이션 모델’과 자체 3차원(3D) 시뮬레이션 플랫폼 ‘옴니버스’를 결합해 피지컬 에이아이 개발 비용을 낮출 수 있다. 현장 테스트, 도로 주행 등을 통해 수집한 데이터가 적더라도 현실과 거의 똑같은 시뮬레이션 영상으로 인공지능을 훈련해, 자율주행과 로봇 기술을 고도화할 수 있는 길을 열었다는 것이다. 이번 발표를 계기로 ‘제2의 기계 시대’가 본격화할 것이란 전망도 나온다. 젠슨 황은 “로봇 공학을 위한 챗지피티(GPT)의 순간이 다가오고 있다”고 말했다. 이런 변화는 한국 기업들에 커다란 기회 요인이 될 전망이다. 자율주행차와 로봇의 ‘두뇌’(피지컬 에이아이)를 개발하는 일이 상대적으로 쉬워지면서 다양한 시도를 해볼 수 있는 환경이 조성됐다. 무엇보다 조선, 자동차, 가전, 반도체를 비롯한 첨단 제조업 기반 시설, 관련 지식과 노하우를 축적해왔다는 점이 대한민국의 큰 강점이다. 피지컬 에이아이 기반의 기기를 실제 작동하게 하고, 나아가 상업화하려면 제조, 기계, 소재 분야 역량이 필수적으로 뒷받침돼야 한다. 미래학자인 정지훈 아시아투지(2G)캐피털 파트너는 “실리콘밸리에서는 최근 기계공학 전공자에 대한 관심이 높아지고 있다”며 “앞으로 에이아이가 물리 세계에서 동작하게 만들어줄 인재, 기술, 기업이 각광받을 가능성이 크다”고 했다. 엔비디아가 제시한 화두를 어떻게 활용하느냐는 다른 이들의 몫이다. 새해를 시작하는 시점에 촉발된 새로운 기회를 한국의 인재, 기업들이 적극적으로 발전시켜 2025년을 혁신과 성공의 해로 만들었으면 하는 바람이다.\"\n",
    "summary = summarize_text(input_text)\n",
    "print(\"요약 결과:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df48dee94c4245df99ed49ef9c60ad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sgjeo\\.conda\\envs\\pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sgjeo\\.cache\\huggingface\\hub\\models--MLP-KTLim--llama-3-Korean-Bllossom-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310a48e92f394ec2bdd81cf851f81f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4695e2153f74e7cbd4b639e26c31ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75064ad8954440e58d52686ed05fab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a1ab7965d6444c802166be61a8d35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9daebc33754898a17804e884ec2adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcbca0930024e209268876e9ef4e18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ca8df60beb4450bdd460114291dbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 모델 ID 및 토크나이저 로드\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz_and_answer(text):\n",
    "    # 시스템 프롬프트 설정\n",
    "    system_prompt = (\n",
    "        \"당신은 유능한 AI 어시스턴트입니다. \"\n",
    "        \"주어진 문장에 대해 주관식 퀴즈와 그에 대한 정답을 생성해주세요.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 입력 설정\n",
    "    user_input = f\"문장: {text}\\n퀴즈와 정답을 생성해주세요.\"\n",
    "\n",
    "    # 메시지 구성\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "    # 프롬프트 생성\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 텍스트 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # 결과 디코딩\n",
    "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 퀴즈\n",
      "젠슨 황 엔비디아 최고경영자(CEO)가 언급한 로봇의 초기 적용 분야는 무엇일까요?\n",
      "\n",
      "### 정답\n",
      "제품 생산 분야\n",
      "\n",
      "### 설명\n",
      "젠슨 황 CEO는 인터뷰에서 로봇이 다양한 분야에 적용될 수 있다고 언급했으며, 그 중에서도 가장 먼저 적용될 분야는 제품 생산 분야라고 말했다.\n"
     ]
    }
   ],
   "source": [
    "text = summary\n",
    "result = generate_quiz_and_answer(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
